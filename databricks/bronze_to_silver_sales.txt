# ===============================
# Configuration
# ===============================

storage_account = "storagedemo01"    
bronze_container = "bronze"
silver_container = "silver"

# ===============================
# READ DATA FROM BRONZE (WILDCARD READ)
# Automatically picks all yearly sales files
# ===============================

bronze_path = (
    f"abfss://{bronze_container}@{storage_account}.dfs.core.windows.net/"
    "sales/AdventureWorks_Sales_*.csv"
)

df_raw = (
    spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv(bronze_path)
)

display(df_raw)

# ===============================
# BASIC PROFILING
# ===============================

print("Total Records:", df_raw.count())
df_raw.printSchema()

# ===============================
# STANDARDIZE COLUMN NAMES
# ===============================
 
from pyspark.sql.functions import col

df = (
    df_raw
    .withColumnRenamed("OrderDate", "order_date")
    .withColumnRenamed("SalesAmount", "sales_amount")
    .withColumnRenamed("ProductKey", "product_key")
    .withColumnRenamed("CustomerKey", "customer_key")
)


# ===============================
# DATA TYPE CASTING (CRITICAL)
# ===============================

from pyspark.sql.functions import to_date

df = (
    df
    .withColumn("order_date", to_date(col("order_date"), "yyyy-MM-dd"))
    .withColumn("sales_amount", col("sales_amount").cast("double"))
    .withColumn("product_key", col("product_key").cast("int"))
    .withColumn("customer_key", col("customer_key").cast("int"))
)


# ===============================
# ️DATA QUALITY CHECKS
# ===============================

# Valid Records
df_valid = df.filter(
    col("order_date").isNotNull() &
    col("sales_amount").isNotNull()
)

# Rejected Records
df_reject = df.subtract(df_valid)

print("Valid Records:", df_valid.count())
print("Rejected Records:", df_reject.count())

-- Rejected data is stored separately for audit/debugging.

# ===============================
# ️️DERIVED COLUMNS (ANALYTICS-READY)
# ===============================
  
from pyspark.sql.functions import year, month

df_final = (
    df_valid
    .withColumn("order_year", year(col("order_date")))
    .withColumn("order_month", month(col("order_date")))
)

# ===============================
# ️️WRITE DATA TO SILVER (PARQUET, PARTITIONED)
# ===============================
  
-- Overwrite is safe because Bronze is immutable

silver_sales_path = (
    f"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/sales/"
)

(
    df_final
    .write
    .mode("overwrite")
    .partitionBy("order_year")
    .parquet(silver_sales_path)
)

# ===============================
# WRITE REJECTED RECORDS 
# ===============================
silver_reject_path = (
    f"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/rejects/sales/"
)

(
    df_reject
    .write
    .mode("overwrite")
    .parquet(silver_reject_path)
)
