# ============================
# NOTEBOOK PARAMETERS (From ADF)
# =============================

dbutils.widgets.text("load_date", "")
dbutils.widgets.text("run_id", "")

load_date = dbutils.widgets.get("load_date")   # e.g. 2024-01-01
run_id = dbutils.widgets.get("run_id")

print("Load Date:", load_date)
print("Run ID:", run_id)

# =================================
# Why this matters
# - Enables audit tracking
# - Enables reprocessing by date
# ================================

# ===============================
# CONFIGURATION
# ==============================

storage_account = "storagedemo01"
bronze_container = "bronze"
silver_container = "silver"

bronze_path = f"abfss://{bronze_container}@{storage_account}.dfs.core.windows.net/sales/"
silver_path = f"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/sales/"
reject_path = f"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/rejects/sales/"
audit_path  = f"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/audit/"

# ===================================
# INCREMENTAL READ (WATERMARK-BASED)
# ===================================

from pyspark.sql.functions import col

df_raw = (
    spark.read
    .option("header", True)
    .csv(f"{bronze_path}AdventureWorks_Sales_*.csv")
)

if load_date:
    df_raw = df_raw.filter(col("OrderDate") >= load_date)


# Only new or changed data using a watermark.


# =====================================
# SCHEMA ENFORCEMENT (NO INFER SCHEMA)
# ====================================

from pyspark.sql.types import *

sales_schema = StructType([
    StructField("OrderDate", DateType(), True),
    StructField("SalesAmount", DoubleType(), True),
    StructField("ProductKey", IntegerType(), True),
    StructField("CustomerKey", IntegerType(), True)
])

df_raw = (
    spark.read
    .schema(sales_schema)
    .option("header", True)
    .csv(f"{bronze_path}AdventureWorks_Sales_*.csv")
)


# ============================
# Why this matters
# - Prevents schema drift
# - Production-safe
# =============================

# ======================
# STANDARDIZATION
# =====================

from pyspark.sql.functions import to_date

df = (
    df_raw
    .withColumnRenamed("OrderDate", "order_date")
    .withColumnRenamed("SalesAmount", "sales_amount")
    .withColumnRenamed("ProductKey", "product_key")
    .withColumnRenamed("CustomerKey", "customer_key")
)

# ======================
# DEDUPLICATION
# =====================

from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window_spec = Window.partitionBy(
    "order_date", "product_key", "customer_key"
).orderBy(col("sales_amount").desc())

df_dedup = (
    df
    .withColumn("rn", row_number().over(window_spec))
    .filter(col("rn") == 1)
    .drop("rn")
)


# Have done deduplicate using business keys to avoid double counting.

# ======================
# DATA QUALITY 
# =====================

from pyspark.sql.functions import when, year, month

df_dq = (
    df_dedup
    .withColumn("dq_order_date_valid", col("order_date").isNotNull())
    .withColumn("dq_sales_amount_valid", col("sales_amount") > 0)
)

df_valid = df_dq.filter(
    col("dq_order_date_valid") &
    col("dq_sales_amount_valid")
)

df_reject = df_dq.subtract(df_valid)

# Bad data is quarantined, not dropped.

# ======================
# DERIVED COLUMNS 
# =====================

df_final = (
    df_valid
    .withColumn("order_year", year(col("order_date")))
    .withColumn("order_month", month(col("order_date")))
    .withColumn("run_id", col("order_year"))  # example lineage
)

# ======================
# IDEMPOTENT WRITE TO SILVER
# =====================

(
    df_final
    .write
    .mode("overwrite")
    .partitionBy("order_year")
    .parquet(silver_path)
)

# ======================
# WRITE REJECTS
# =====================

(
    df_reject
    .write
    .mode("append")
    .parquet(reject_path)
)

# ======================
# AUDIT LOGGING
# =====================

from pyspark.sql.functions import current_timestamp

audit_df = spark.createDataFrame(
    [
        (
            run_id,
            load_date,
            df_raw.count(),
            df_final.count(),
            df_reject.count()
        )
    ],
    ["run_id", "load_date", "records_read", "records_written", "records_rejected"]
).withColumn("processed_at", current_timestamp())

audit_df.write.mode("append").parquet(audit_path)

